# Classifying images {#image_classification}

Our image classification example will differentiate ... not dogs and cats, not different dog breeds, but ...different species of birds. We start from a model pre-trained on [ImageNet](http://www.image-net.org/), which contains abundantly many photos of birds (and other animals you wouldn't know even existed).

Concretely, for the pre-trained model we'll use a Resnet, one of several classic computer vision models provided by `torchvision`, and attach our own classification layer on top. If you are looking for how to code a convolutional neural network from scratch, you can pick up related information in the following chapter on image segmentation, as well as that on generative adversarial networks (GANs).

## Data loading and transformation

The example dataset used here is available on Kaggle (<https://www.kaggle.com/gpiosenka/100-bird-species/data>). It is very "non-noisy", which is why, the number of classes notwithstanding (130!), accuracy will turn out to be very good.

```{r}
library(torch)
library(torchvision)
library(dplyr)
```

```{r}
# from: https://www.kaggle.com/gpiosenka/100-bird-species/data
data_dir = 'data/bird_species'
```

The data set being so clean, we'll want to introduce random noise (*data augmentation*) on the training set to enhance model resiliency.

In `torchvision`, data augmentation steps are added as part of an *image processing pipeline* that also takes care of resizing and/or cropping images, converting them to `torch` tensors, and possibly, normalizing them according to the model's expectations. Here they are, deterministic on validation and test sets, but including random components for the training set:

```{r}
train_transforms <- function(img) {
  img %>%
    transform_random_resized_crop(size = c(224, 224)) %>%
    transform_color_jitter() %>%
    transform_random_horizontal_flip() %>%
    transform_to_tensor() %>%
    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225))
}

valid_transforms <- function(img) {
  img %>%
    transform_resize(256) %>%
    transform_center_crop(224) %>%
    transform_to_tensor() %>%
    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225))
}

test_transforms <- valid_transforms

# TBD remove later
target_transform = function(x) {
  x <- torch_tensor(x, dtype = torch_long())
  x$squeeze(1)
}
```

`image_folder_dataset` is a subtype of dataset that encapsulates information about where the images reside, and what transformations to apply. Here, we create such a dataset for each of training, validation and test set:

```{r}
train_ds <- image_folder_dataset(
  file.path(data_dir, "train"),
  transform = train_transforms,
  target_transform = target_transform)

valid_ds <- image_folder_dataset(
  file.path(data_dir, "valid"),
  transform = valid_transforms,
  target_transform = target_transform)

test_ds <- image_folder_dataset(
  file.path(data_dir, "test"),
  transform = test_transforms)

```

`image_folder_dataset` objects expect the different classes of images to reside each in their own folder. In our example, this is in fact the case; for example, here is the directory layout for the first three classes in the test set:

    data/test/ALBATROSS/
     - data/test/ALBATROSS/1.jpg
     - data/test/ALBATROSS/2.jpg
     - data/test/ALBATROSS/3.jpg
     - data/test/ALBATROSS/4.jpg
     - data/test/ALBATROSS/5.jpg
     
    data/test/'ALEXANDRINE PARAKEET'/
     - data/test/'ALEXANDRINE PARAKEET'/1.jpg
     - data/test/'ALEXANDRINE PARAKEET'/2.jpg
     - data/test/'ALEXANDRINE PARAKEET'/3.jpg
     - data/test/'ALEXANDRINE PARAKEET'/4.jpg
     - data/test/'ALEXANDRINE PARAKEET'/5.jpg
     
     data/test/'AMERICAN BITTERN'/
     - data/test/'AMERICAN BITTERN'/1.jpg
     - data/test/'AMERICAN BITTERN'/2.jpg
     - data/test/'AMERICAN BITTERN'/3.jpg
     - data/test/'AMERICAN BITTERN'/4.jpg
     - data/test/'AMERICAN BITTERN'/5.jpg

From those specifications, `dataloaders` are created. These objects, in addition to *what* to load and which transformations to apply, know things like: how many items to load per batch, whether they should be shuffled, and whether to parallelize the transformations.

```{r}
train_dl <- dataloader(train_ds, batch_size = 16, shuffle = TRUE)
valid_dl <- dataloader(valid_ds, batch_size = 16)
test_dl <- dataloader(test_ds, batch_size = 16)
```

How many items are there in each set?

```{r}
train_dl$.length() # 1065
valid_dl$.length() # 41
test_dl$.length()  # 41
```

Datasets know what classes there are:

```{r}
class_names <- train_ds$classes
class_names
```

     [1] "ALBATROSS"                  "ALEXANDRINE PARAKEET"     
     [3] "AMERICAN BITTERN"           "AMERICAN GOLDFINCH"       
     [5] "AMERICAN KESTREL"           "AMERICAN REDSTART"        
     [7] "ANHINGA"                    "ANNAS HUMMINGBIRD"        
     [9] "BALD EAGLE"                 "BALTIMORE ORIOLE"         
     [11] "BANANAQUIT"                "BAR-TAILED GODWIT"        
     [13] "BARN OWL"                  "BARN SWALLOW"             
     [15] "BAY-BREASTED WARBLER"      "BELTED KINGFISHER"        
     [17] "BIRD OF PARADISE"          "BLACK FRANCOLIN"          
     [19] "BLACK SKIMMER"             "BLACK-CAPPED CHICKADEE"   
     [21] "BLACK-NECKED GREBE"        "BLACKBURNIAM WARBLER"     
     [23] "BLUE HERON"                "BOBOLINK"                 
     [25] "BROWN THRASHER"            "CACTUS WREN"              
     [27] "CALIFORNIA CONDOR"         "CALIFORNIA GULL"          
     [29] "CALIFORNIA QUAIL"          "CAPE MAY WARBLER"         
     [31] "CHARA DE COLLAR"           "CHIPPING SPARROW"         
     [33] "CINNAMON TEAL"             "COCK OF THE  ROCK"        
     [35] "COCKATOO"                  "COMMON LOON"              
     [37] "COMMON POORWILL"           "COMMON STARLING"          
     [39] "COUCHS KINGBIRD"           "CRESTED AUKLET"           
     [41] "CRESTED CARACARA"          "CROW"                     
     [43] "CROWNED PIGEON"            "CURL CRESTED ARACURI"     
     [45] "DARK EYED JUNCO"           "DOWNY WOODPECKER"         
     [47] "EASTERN BLUEBIRD"          "EASTERN ROSELLA"          
     [49] "EASTERN TOWEE"             "ELEGANT TROGON"           
     [51] "EMPEROR PENGUIN"           "EVENING GROSBEAK"         
     [53] "FLAME TANAGER"             "FLAMINGO"                 
     [55] "FRIGATE"                   "GLOSSY IBIS"              
     [57] "GOLD WING WARBLER"         "GOLDEN CHLOROPHONIA"      
     [59] "GOLDEN EAGLE"              "GOLDEN PHEASANT"          
     [61] "GOULDIAN FINCH"            "GRAY CATBIRD"             
     [63] "GRAY PARTRIDGE"            "GREY PLOVER"              
     [65] "HAWAIIAN GOOSE"            "HOODED MERGANSER"         
     [67] "HOOPOES"                   "HOUSE FINCH"              
     [69] "HOUSE SPARROW"             "HYACINTH MACAW"           
     [71] "INDIGO BUNTING"            "JABIRU"                   
     [73] "LARK BUNTING"              "LILAC ROLLER"             
     [75] "LONG-EARED OWL"            "MALLARD DUCK"             
     [77] "MANDRIN DUCK"              "MARABOU STORK"            
     [79] "MOURNING DOVE"             "MYNA"                     
     [81] "NICOBAR PIGEON"            "NORTHERN CARDINAL"        
     [83] "NORTHERN FLICKER"          "NORTHERN GOSHAWK"         
     [85] "NORTHERN MOCKINGBIRD"      "OSTRICH"                  
     [87] "PAINTED BUNTIG"            "PARADISE TANAGER"         
     [89] "PARUS MAJOR"               "PEACOCK"                  
     [91] "PELICAN"                   "PEREGRINE FALCON"         
     [93] "PINK ROBIN"                "PUFFIN"                   
     [95] "PURPLE FINCH"              "PURPLE GALLINULE"         
     [97] "PURPLE MARTIN"             "QUETZAL"                  
     [99] "RAINBOW LORIKEET"          "RED FACED CORMORANT"      
    [101] "RED HEADED WOODPECKER"     "RED THROATED BEE EATER"   
    [103] "RED WINGED BLACKBIRD"      "RED WISKERED BULBUL"      
    [105] "RING-NECKED PHEASANT"      "ROADRUNNER"               
    [107] "ROBIN"                     "ROUGH LEG BUZZARD"        
    [109] "RUBY THROATED HUMMINGBIRD" "SAND MARTIN"              
    [111] "SCARLET IBIS"              "SCARLET MACAW"            
    [113] "SNOWY EGRET"               "SPLENDID WREN"            
    [115] "STORK BILLED KINGFISHER"   "STRAWBERRY FINCH"         
    [117] "TEAL DUCK"                 "TIT MOUSE"                
    [119] "TOUCHAN"                   "TRUMPTER SWAN"            
    [121] "TURKEY VULTURE"            "TURQUOISE MOTMOT"         
    [123] "VARIED THRUSH"             "VENEZUELIAN TROUPIAL"     
    [125] "VERMILION FLYCATHER"       "VIOLET GREEN SWALLOW"     
    [127] "WESTERN MEADOWLARK"        "WILSONS BIRD OF PARADISE" 
    [129] "WOOD DUCK"                 "YELLOW HEADED BLACKBIRD"  
    > 

Next, let's view a few images from the test set. We can retrieve the first batch -- images and corresponding classes -- by creating an iterator from the `dataloader` and calling `next()` on it:

```{r}
batch <- train_dl$.iter()$.next()
```

`batch` is a list, the first item being the image tensors ...

```{r}
batch[[1]]$size()
```

    [1]  16   3 224 224

... and the second, the classes:

```{r}
batch[[2]]$size()
```

    [1] 16

The classes are coded as integers, to be used as indexes into the vector of class names. We'll use those for labeling the images.

```{r}
classes <- batch[[2]]
classes
```

    torch_tensor 
       3
      46
      67
      52
      88
     112
      76
     111
     109
      70
      40
      72
      75
      53
      27
      90
    [ CPULongType{16} ]

Now to visualization. The image tensors are of shape `batch_size x num_channels x height x width`. Since we want to use `as.raster` for plotting, we need to reshape images such that channels come last. Here are the first sixteen images:

```{r, fig.asp = 1, fig.width = 8}
library(dplyr)

images <- as_array(batch[[1]]) %>% aperm(perm = c(1, 3, 4, 2))
mean <- c(0.485, 0.456, 0.406)
std <- c(0.229, 0.224, 0.225)
images <- std * images + mean
images <- images * 255
images[images > 255] <- 255
images[images < 0] <- 0

par(mfcol = c(4,4), mar = rep(1, 4))

images %>%
  purrr::array_tree(1) %>%
  purrr::set_names(class_names[as_array(classes)]) %>%
  purrr::map(as.raster, max = 255) %>%
  purrr::iwalk(~{plot(.x); title(.y)})
```

```{r, eval = TRUE}
knitr::include_graphics("images/image_classif_birds.png")
```

## Model

The backbone of our model is a pre-trained instance of Resnet.

```{r}
model <- model_resnet18(pretrained = TRUE)
```

We will modify the model's output layer to distinguish between our 130 bird classes, instead of the 1000 ImageNet classes it was trained for. This means we only need to train a single layer -- the one we're going to add. We *could* perform backpropagation through the complete model, trying to fine-tune Resnet's weights as well, but that would have a significant effect on training time. (Alternatively, we could try to fine-tune just a few of Resnet's weights, those located in the layers directly preceding the output.)

```{r}
model$parameters %>% 
  purrr::walk(function(param) param$requires_grad <- FALSE)
```

To replace the output layer, the model is just modified in-place:

```{r}
num_features <- model$fc$in_features

model$fc <- nn_linear(in_features = num_features, out_features = length(class_names))
```

Now put the modified model on the GPU:

```{r}
device <- if (cuda_is_available()) torch_device("cuda:0") else "cpu"

model <- model$to(device = device)
```

## Training

For training, we use cross entropy loss and stochastic gradient descent.

```{r}
criterion <- nn_cross_entropy_loss()

optimizer <- optim_sgd(model$parameters, lr = 0.001, momentum = 0.9)
```

We set the learning rate to 0.1, but that is just a formality. As became widely known due to [fast.ai's deep learning lectures](), it always makes sense to spend some time upfront to determine a good learning rate, and then during training, evolve the learning rate according to some established algorithm. While out-of-the-box, `torch` does not provide a tool like fast.ai's learning rate finder, the logic is straightforward to implement, and sample code is given on Sylvain Gugger's blog.

Algorithms like one-cycle learning [@abs-1708-07120], cyclical learning rates [@Smith15a], or cosine annealing with warm restarts [@LoshchilovH16a] are, however, implemented in `torch`, and we'll make use of `lr_one_cycle` once we've determined an appropriate value for the required parameter `max_lr`.

Here is how to find a good learning rate, translated to R from [Sylvain Gugger's post](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html):

```{r}
# ported from: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html

losses <- c()
log_lrs <- c()

find_lr <- function(init_value = 1e-8, final_value = 10, beta = 0.98) {

  num <- train_dl$.length()
  mult = (final_value/init_value)^(1/num)
  lr <- init_value
  optimizer$param_groups[[1]]$lr <- lr
  avg_loss <- 0
  best_loss <- 0
  batch_num <- 0

  for (b in enumerate(train_dl)) {

    batch_num <- batch_num + 1
    optimizer$zero_grad()
    output <- model(b[[1]]$to(device = devcie))
    loss <- criterion(output, b[[2]]$to(device = device))

    #Compute the smoothed loss
    avg_loss <- beta * avg_loss + (1-beta) * loss$item()
    smoothed_loss <- avg_loss / (1 - beta^batch_num)
    #Stop if the loss is exploding
    if (batch_num > 1 && smoothed_loss > 4 * best_loss) break
    #Record the best loss
    if (smoothed_loss < best_loss || batch_num == 1) best_loss <- smoothed_loss

    #Store the values
    losses <- c(losses, smoothed_loss)
    log_lrs <- c(log_lrs, (log(lr, 10)))

    loss$backward()
    optimizer$step()

    #Update the lr for the next step
    lr <- lr * mult
    optimizer$param_groups[[1]]$lr <- lr
  }
}

find_lr()
```

```{r}
df <- data.frame(log_lrs = log_lrs, losses = losses)
library(ggplot2)
ggplot(df, aes(log_lrs, losses)) + geom_point(size = 1)
```

The best learning rate is not the exact one where loss is at a minimum, instead, it should be picked somewhat earlier on the curve, while loss still decreases. We'll try 0.05 here.

`OneCycleLR` will then vary the learning rate continuously, performing just a single ramp-up and a single ramp-down over the whole training period:

```{r}
num_epochs = 10
scheduler <- optimizer %>%
  lr_one_cycle(max_lr = 0.05, epochs = num_epochs, steps_per_epoch = train_dl$.length())
```

Now we train for ten epochs. Every epoch, we iterate over both training and validation sets, performing optimization on the training set while just calculating accuracy on the test set. Note that `scheduler$step()` has to be called explicitly after each batch, and it has to be called *after* `optimizer$step()`.

```{r}
for (epoch in 1:num_epochs) {

  model$train()
  train_losses <- c()

  for (b in enumerate(train_dl)) {
    optimizer$zero_grad()
    output <- model(b[[1]]$to(device = device))
    loss <- criterion(output, b[[2]]$to(device = device))
    loss$backward()
    optimizer$step()
    scheduler$step()
    train_losses <- c(train_losses, loss$item())
  }

  model$eval()
  valid_losses <- c()

  for (b in enumerate(valid_dl)) {
    output <- model(b[[1]])
    loss <- criterion(output, b[[2]]$to(device = device))
    valid_losses <- c(valid_losses, loss$item())
  }

  cat(sprintf("Loss at epoch %d: training: %3f, validation: %3f\n", epoch, mean(train_losses), mean(valid_losses)))
}
```

## Performance on the test set

Finally, we calculate accuracy on the test set:

```{r}
model$eval()

test_losses <- c()
total <- 0
correct <- 0

for (b in enumerate(test_dl)) {
  output <- model(b[[1]]$to(device = device))
  labels <- b[[2]]$to(device = "cuda")
  loss <- criterion(output, labels)
  test_losses <- c(test_losses, loss$item())
  # torch_max returns a list, with position 1 containing the values
  # and position 2 containing the respective indices
  predicted <- torch_max(output$data(), dim = 2)[[2]]
  total <- total + labels$size(1)
  # add number of correct classifications in this batch to the aggregate
  correct <- correct + (predicted == labels)$sum()$item()
}

mean(test_losses)

```

    tbd

```{r}
test_accuracy <-  correct/total
test_accuracy

```

    tbd
